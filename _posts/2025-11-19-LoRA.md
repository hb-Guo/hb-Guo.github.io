---
title: "LoRA微调"
date: 2025-11-19
permalink: /posts/2025/11/lora/
categories: [概念]
tags: [LLM, 微调]
excerpt: "大模型参数微调相关"
#bundle exec jekyll serve
---

# LoRA-Low-Rank Adaptation（低秩适应）

流行的参数高效微调技术，主要应用于LLM和扩散模型的微调。

## 核心原理

传统的全量微调（Full Fine-Tuning）需要修改模型所有参数（几亿到几百亿），耗费大量显存和时间。
LoRA 的聪明之处在于：**它不改原模型的参数，而是冻结原始权重，只训练一小部分额外的“低秩矩阵”**来实现微调效果。

### 
![]({{"../images/2025-11-19-11-43-53.png"|relative_url}})
