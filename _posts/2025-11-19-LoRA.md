---
title: "LoRA微调"
date: 2025-11-19
permalink: /posts/2025/11/lora/
categories: [概念]
tags: [LLM, 微调, LoRA, 旁路矩阵, 高斯分布, 正态分布]
excerpt: "大模型参数微调相关"
#bundle exec jekyll serve
---

# LoRA-Low-Rank Adaptation（低秩适应）

流行的参数高效微调技术，主要应用于LLM和扩散模型的微调。*旁路矩阵*

## 核心原理

传统的全量微调（Full Fine-Tuning）需要修改模型所有参数（几亿到几百亿），耗费大量显存和时间。
LoRA 的聪明之处在于：**它不改原模型的参数，而是冻结原始权重，只训练一小部分额外的“低秩矩阵”**来实现微调效果。

### 技术细节

![]({{"../images/2025-11-19-12-26-59.png"|relative_url}})

在Transformer的权重旁边添加两个低秩的小矩阵A和B，先降维后升维，预训练模型的参数固定。模型的输入输出维度不变，输出时， $\Delta W = A × B$ ，将该值与原始矩阵相加即可得到最终的权重。

**初始化**：采用高斯分布初始化A，采用0矩阵初始化B

$$
W=W_0+\Delta W=W_0+BA,B \in \mathbb{R}^{d×r}, A \in \mathbb{R}^{r×k}, r \ll min(d,k)
$$

## 额外注意的

对于一般任务，$r=1,2,4,8$ 足够了，对领域差距较大额任务可能需要更大的 $r$。

$r$ 越大并不一定能提升微调的效果。