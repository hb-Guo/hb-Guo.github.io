---
title: "ST-LLM+: Graph Enhanced Spatio-Temporal Large  Language Models for Traffic Prediction."
date: 2025-11-19
permalink: /posts/2025/11/paper3_tkde/
categories: [论文阅读]
tags: [时空预测, LLM, 城市计算, 图学习, 少样本, 零样本, LoRA, 参数微调, 全局-局部依赖]
excerpt: "图增强的时空预测大模型"
---

# ST-LLM+: Graph Enhanced Spatio-Temporal Large  Language Models for Traffic Prediction

## 动机

让LLM既有原有的预训练得来的通用时序先验能力，又能够图邻接矩阵使用图注意力机制做空间注意。微调极少的参数，保证稀疏数据场景也能有很好的泛化能力。 

## 主要创新

🧠 ST-LLM+ 的三大创新点：

1. **图增强注意力（Graph-based Attention）**

    使用交通网络的邻接矩阵（如共享道路的站点之间更接近），
    作为 LLM 注意力机制的 Mask。

    让模型不仅关注时间序列，还关注空间邻近关系

    捕捉交通网络中复杂的传播结构，如拥堵扩散

2. **部分冻结图注意力（PFGA）**

    将 LLM 的前几层冻结，只训练后几层：

    冻结层：保持 LLM 预训练的全局序列建模能力

    解冻层：加入图注意力，学习交通领域的本地空间依赖

    这是对以前 ST-LLM 中 PFA 的升级。

3. **LoRA 轻量化微调**

    对解冻的注意力层加入 LoRA 低秩矩阵进行微调：

    大幅减少训练参数（从 50% 降到 ~4–6%）

    不牺牲模型效果

    加快训练速度

![]({{"../images/2025-11-19-15-42-51.png"|relative_url}})


**核心模块是PFGA模块** 冻结前F层，微调后U层。